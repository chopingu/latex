\section{Introduction}
\label{sec:introduction} 
Deep CNNs, such as VGG16 or ResNet50 already deliver great performance \citep{vgg_simonyan2014very,restnet_DBLP:journals/corr/HeZRS15}. Yet the increasing number of layers in such models comes at the cost of severely increased computational complexity, resulting in the need for power-hungry hardware \citep{comp_lim_DBLP:journals/corr/abs-2007-05558, comp_lim2_DBLP:journals/corr/JinYIK16}. An example of a model that behaves extremely poorly in this regard is a big transformer with neural architecture search \citep{DBLP:journals/corr/abs-1906-02243}. Clearly, training and running these models is not just a matter of financial cost, but also environmental impact. Yet, deep learning models are often only evaluated concerning standard performance metrics like accuracy or inference time, but not energy consumption or ecological footprint.  
While advancements in the research field often take precedence over energy considerations, this trade-off between performance and environmental impact should still be taken into account, despite the difficulty in accurately reporting these metrics due to a lack of appropriate tools \citep{impact_DBLP:journals/corr/abs-1910-09700, estimation_of_energy_GARCIAMARTIN201975}.

\looseness=-1
In light of these motivations, we developed a pipeline to collect model energy data and to approximate a model's energy consumption, before training and without the need for a direct energy measurement. Not only would this approach be more efficient and environmentally friendly, but it would also allow deep learning engineers to strike a balance between performance and environmental impact with greater ease. To achieve this goal, we attempt to compute the energy consumption of various deep learning architectures as the sum of their layer-wise estimated energies.


\textbf{Related work.} While previous publications in this field are rather scarce, some notable contributions helped tremendously with our research. The \emph{experiment-impact-tracker} is a python package that allows the user to measure their models and report their carbon footprint \citep{impact-tracker_DBLP:journals/corr/abs-2002-05651}. Another python package and integral part of our research is \emph{codecarbon}, which also allows for accurate CPU energy measurements and carbon reporting \citep{codecarbon}.  Additionally, \citet{impact_DBLP:journals/corr/abs-1910-09700} created \emph{ML CO2 Impact}, a web tool to calculate a models CO2 impact based on key parameters. Furthermore, in \citet{NeuralPower_DBLP:journals/corr/abs-1710-05420} the authors also  attempt to estimate the energy consumption of CNNs through layer-wise estimates. Unfortunately, their work shows some limitations as the energy consumption was measured from a GPU with a fixed clock speed, which is not realistic for real-world use. Moreover, the authors utilize hardware features in their models, making their tool more hardware-dependent. The authors of \citet{misnomer} further motivate our cause by promoting the usage of concrete real-world metrics, and show that no single cost indicator is sufficient and that incomplete cost reporting can have a direct environmental impact. \\

\looseness=-1
\textbf{Our Contribution.} \textbf{(1)} We release a high-quality data set on the energy consumption of various models and layer types, along with a novel and modular data-collection process, built to make our research hardware independent (Sec.~\ref{sec:data-collection}). \textbf{(2)} We created a collection of predictors for different layer types, forming a simple energy-prediction baseline for the energy consumption of multiple Deep Learning (DL) architectures (Sec.~\ref{sec:simple-baseline}). \textbf{(3)} We conducted a complex analysis of the predictive capabilities of various feature sets, offering valuable insights into the energy behavior of different architectures and layer types (Sec.~\ref{sec:results}).